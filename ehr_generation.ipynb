{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "local_dir = ''\n",
    "data_dir = ''\n",
    "sys.path.insert(0, os.path.join(local_dir, 'CPRD_Cut22'))\n",
    "\n",
    "from utils.yaml_act import yaml_load\n",
    "from utils.arg_parse import arg_paser\n",
    "from CPRD.config.spark import spark_init, read_parquet, read_txt, read_csv\n",
    "import pyspark.sql.functions as F\n",
    "from CPRD.functions import tables, merge,risk_prediction, modalities, MedicalDictionary, risk_prediction, predictor_extractor\n",
    "from CPRD.base.table import Patient,Practice,Clinical, Diagnosis, Therapy, Hes, Consultation, Proc_HES\n",
    "from CPRD.functions import merge\n",
    "from utils.utils import save_obj, load_obj\n",
    "from CPRD.functions.cohort_select import Cohort\n",
    "\n",
    "from CPRD.config.utils import cvt_str2time\n",
    "from CPRD.config.utils import check_time\n",
    "from CPRD.config.utils import RangeExtract\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "args = dotdict({'params': os.path.join(local_dir, 'CPRD_Cut22/config/config.yaml')})\n",
    "params = yaml_load(args.params)\n",
    "spark_params = params['pyspark']\n",
    "spark = spark_init(spark_params)\n",
    "file = params['file_path']\n",
    "data_params = params['params']\n",
    "pheno_dict = load_obj(file['PhenoMaps'])\n",
    "save_path = os.path.join(os.path.join(local_dir, 'EHR_AGE/UKB_EHR_TIME'))\n",
    "patid_path = 'data/ukbiobank_phenoage.csv'\n",
    "ukb_data_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patient = spark.sqlContext.read.option(\"header\", \"true\")\\\n",
    "    .option(\"maxColumns\", 10000000)\\\n",
    "    .csv(patid_path)\n",
    "main = spark.sqlContext.read.option(\"header\", \"true\")\\\n",
    "    .option(\"maxColumns\", 10000000)\\\n",
    "    .csv(ukb_data_path)\\\n",
    "    .select(['eid', '`34-0.0`', '`52-0.0`', '`53-0.0`', '`53-1.0`', '`53-2.0`', '`53-3.0`','`40000-0.0`'])\\\n",
    "    .withColumnRenamed('eid', 'patid')\n",
    "\n",
    "main = main.join(patient, on='patid', how='inner')\n",
    "\n",
    "month_mapping = {\n",
    "    'January': '01', 'February': '02', 'March': '03', 'April': '04',\n",
    "    'May': '05', 'June': '06', 'July': '07', 'August': '08',\n",
    "    'September': '09', 'October': '10', 'November': '11', 'December': '12'\n",
    "}\n",
    "month_map_expr = F.create_map([F.lit(x) for x in sum(month_mapping.items(), ())])\n",
    "\n",
    "# Concatenate year of birth and month of birth to create date of birth (DOB)\n",
    "main = main.withColumn('month_numeric', month_map_expr[F.col('`52-0.0`')])\n",
    "main = main.withColumn(\"dob\", F.to_date(F.concat(F.col('`34-0.0`'), F.lit('-'), F.col('month_numeric'), F.lit('-01')), \"yyyy-MM-dd\"))\n",
    "# Rename columns\n",
    "main = main.withColumnRenamed('53-0.0', 'doa0')\\\n",
    "           .withColumnRenamed('53-1.0', 'doa1')\\\n",
    "           .withColumnRenamed('53-2.0', 'doa2')\\\n",
    "           .withColumnRenamed('53-3.0', 'doa3')\\\n",
    "           .withColumnRenamed('40000-0.0', 'dod')\n",
    "\n",
    "# Convert 'doa0', 'doa1', and 'dod' to date type\n",
    "main = main.withColumn(\"doa0\", F.to_date(F.col(\"doa0\"), \"yyyy-MM-dd\"))\\\n",
    "           .withColumn(\"doa1\", F.to_date(F.col(\"doa1\"), \"yyyy-MM-dd\"))\\\n",
    "           .withColumn(\"doa2\", F.to_date(F.col(\"doa2\"), \"yyyy-MM-dd\"))\\\n",
    "           .withColumn(\"doa3\", F.to_date(F.col(\"doa3\"), \"yyyy-MM-dd\"))\\\n",
    "           .withColumn(\"dod\", F.to_date(F.col(\"dod\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "main = main.drop('34-0.0', '52-0.0', '53-0.0', '53-1.0', '53-2.0', '53-3.0', '40000-0.0', 'month_numeric').cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use GP+HES+First recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses = read_parquet(spark.sqlContext, ukb_data_path+'/diagnoses.parquet')\\\n",
    "    .select(['patid', 'eventdate', 'ICD'])\\\n",
    "    .withColumnRenamed('ICD', 'code').dropna()\n",
    "first_record = read_parquet(spark.sqlContext, ukb_data_path+'/first_record.parquet').withColumnRenamed('eid', 'patid')\\\n",
    "    .withColumnRenamed('ICD', 'code')\n",
    "diagnoses = diagnoses.union(first_record)\n",
    "\n",
    "medications = read_txt(spark.sc, spark.sqlContext, path=ukb_data_path+'/processed_gp_scripts.txt')\\\n",
    "    .withColumn('eventdate', F.to_date('issue_date' ,\"dd/MM/yyyy\"))\\\n",
    "    .withColumnRenamed('mapped_bnf_code', 'code')\\\n",
    "    .withColumnRenamed('eid', 'patid')\\\n",
    "    .select(['patid', 'eventdate', 'code']).dropna()\n",
    "\n",
    "procedure = read_txt(spark.sc, spark.sqlContext, path=ukb_data_path+'/merged_hesin_oper.txt')\\\n",
    "    .withColumn('eventdate', F.to_date('admidate' ,\"dd/MM/yyyy\"))\\\n",
    "    .withColumnRenamed('oper4', 'code')\\\n",
    "    .withColumnRenamed('eid', 'patid')\\\n",
    "    .select(['patid', 'eventdate', 'code']).dropna()\n",
    "data = medications.union(procedure).union(diagnoses).dropna()\n",
    "\n",
    "# Define the enevt date boundaries\n",
    "start_date = \"1950-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "\n",
    "# ----- Restrict the data DataFrame by eventdate -----\n",
    "data = data.filter((F.col(\"eventdate\") >= F.lit(start_date)) & (F.col(\"eventdate\") <= F.lit(end_date))).cache()\n",
    "\n",
    "# end_rows = main.select(\n",
    "#     \"patid\",\n",
    "#     F.to_date(F.col(\"Date of Attendance\"), \"yyyy-MM-dd\").alias(\"eventdate\"),\n",
    "#     F.lit('end').alias(\"code\")\n",
    "# )\n",
    "# end_rows.show()\n",
    "# data = data.unionByName(end_rows)\n",
    "# Create new rows for each patient using the Date of Attendance and baseline_age (converted to string)\n",
    "# end_rows = main.select(\n",
    "#     \"patid\",\n",
    "#     F.to_date(F.col(\"Date of Attendance\"), \"yyyy-MM-dd\").alias(\"eventdate\"),\n",
    "#     F.col(\"Chronological Age\").cast(\"string\").alias(\"code\")\n",
    "# )\n",
    "# end_rows.show()\n",
    "# data = data.unionByName(end_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_attendance = data.join(main.select(\"patid\", \"Date of Attendance\"), on=\"patid\", how=\"inner\")\n",
    "# Filter records to include only events that occur before the Date of Attendance\n",
    "filtered_data = data_with_attendance.filter(F.col(\"eventdate\") < F.col(\"Date of Attendance\"))\n",
    "\n",
    "# For each patient, choose the latest event date before Date of Attendance\n",
    "window_spec = Window.partitionBy(\"patid\").orderBy(F.col(\"eventdate\").desc())\n",
    "last_event = filtered_data.withColumn(\"rank\", F.row_number().over(window_spec))\\\n",
    "                          .filter(F.col(\"rank\") == 1)\\\n",
    "                          .select(\"patid\", F.col(\"eventdate\").alias(\"last_eventdate\"))\n",
    "\n",
    "# Join with main again to ensure we have Date of Attendance available\n",
    "event_gap = last_event.join(main.select(\"patid\", \"Date of Attendance\"), on=\"patid\", how=\"inner\")\n",
    "\n",
    "# Compute the gap in years between Date of Attendance and last event date, rounded down\n",
    "event_gap = event_gap.withColumn(\"gap_years\",F.floor(F.months_between(\"Date of Attendance\", \"last_eventdate\") / 12))\n",
    "\n",
    "# Create end_rows with gap_years as the code column, renaming Date of Attendance to eventdate\n",
    "end_rows = event_gap.withColumn(\"code\", F.col(\"gap_years\").cast(\"string\"))\\\n",
    "                    .select(\"patid\", \"Date of Attendance\", \"code\")\\\n",
    "                    .withColumnRenamed(\"Date of Attendance\", \"eventdate\")\n",
    "\n",
    "# *** FIX: Convert the 'eventdate' column in end_rows to a date type ***\n",
    "end_rows = end_rows.withColumn(\"eventdate\", F.to_date(F.col(\"eventdate\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Now perform the union so that both data and end_rows have eventdate as DateType\n",
    "data = data.unionByName(end_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = main.withColumnRenamed('PhenoAge', 'label').withColumnRenamed('Chronological Age', 'baseline_age')\n",
    "cohort = main.select('patid', 'dob', 'Date of Attendance', 'label')\n",
    "behrt_formater = predictor_extractor.BEHRTextraction()\n",
    "EHR = behrt_formater.format_behrt(data, cohort, col_entry='Date of Attendance', col_yob='dob', age_col_name='age', col_code='code', unique_in_months=6).dropna().cache()\n",
    "EHR = EHR.join(main.select('patid', 'Age_range', 'baseline_age'), on='patid', how='inner')\n",
    "EHR.write.mode(\"overwrite\").parquet(os.path.join(save_path))\n",
    "EHR.show(), EHR.select('code').show(), EHR.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
